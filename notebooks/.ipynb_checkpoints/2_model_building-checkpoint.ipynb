{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutinal Neural Network for Classifying Cars\n",
    "\n",
    "### Background\n",
    "Stanford AI has developed a dataset of cars with make, model and year. The aim of this project is to classify cars as accurately as possible using a convolutional neural network. We will use the Keras package with Tensorflow backend to run model training, and we will validate and evaluate the accuracy of the model based on the parameters.\n",
    "\n",
    "### Contents\n",
    "1. Build the classifier layers\n",
    "2. Load training and test data\n",
    "3. Run training\n",
    "4. Evaluate model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\hmakharia.consultant\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hmakharia.consultant\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hmakharia.consultant\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hmakharia.consultant\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hmakharia.consultant\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hmakharia.consultant\\.conda\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\hmakharia.consultant\\.conda\\envs\\quant\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hmakharia.consultant\\.conda\\envs\\quant\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hmakharia.consultant\\.conda\\envs\\quant\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hmakharia.consultant\\.conda\\envs\\quant\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hmakharia.consultant\\.conda\\envs\\quant\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hmakharia.consultant\\.conda\\envs\\quant\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model A: Classifying Make and Model\n",
    "<b>Aim</b>: Classify a car as part of the 196 classes in our dataset in terms of both make and model year. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the Classifier\n",
    "\n",
    "We will use 3 convolution layers for the sake of computing power, with all layers using the relu activation function. We use this function because of it's non-linear (compared to sigmoid, for example, which can cause neurons to 'vanish'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "img_pixels = 256\n",
    "n_filters = 32\n",
    "layer_nodes = 512\n",
    "batchsize = 616\n",
    "epochs = 50\n",
    "steps_per_epoch = 100\n",
    "validation_steps = 50\n",
    "poolsize = (2,2)\n",
    "kernelsize = (3,3)\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "car_classifier = Sequential()\n",
    "#Adding 1st Convolution and Pooling Layer\n",
    "car_classifier.add(Conv2D(n_filters,kernel_size=kernelsize,input_shape=(img_pixels,img_pixels,3),activation='relu'))\n",
    "car_classifier.add(MaxPool2D(pool_size=poolsize))\n",
    "car_classifier.add(Dropout(dropout))\n",
    "#Adding 2nd Convolution and Pooling Layer\n",
    "car_classifier.add(Conv2D(n_filters,kernel_size=kernelsize,activation='relu'))\n",
    "car_classifier.add(MaxPool2D(pool_size=poolsize))\n",
    "car_classifier.add(Dropout(dropout))\n",
    "#Adding 3rd Convolution and Pooling Layer\n",
    "car_classifier.add(Conv2D(n_filters,kernel_size=kernelsize,activation='relu'))\n",
    "car_classifier.add(MaxPool2D(pool_size=poolsize))\n",
    "car_classifier.add(Dropout(dropout))\n",
    "#Adding 4th Convolution and Pooling Layer\n",
    "car_classifier.add(Conv2D(n_filters,kernel_size=kernelsize,activation='relu'))\n",
    "car_classifier.add(MaxPool2D(pool_size=poolsize))\n",
    "car_classifier.add(Dropout(dropout))\n",
    "#Adding 5th Convolution and Pooling Layer\n",
    "car_classifier.add(Conv2D(n_filters,kernel_size=kernelsize,activation='relu'))\n",
    "car_classifier.add(MaxPool2D(pool_size=poolsize))\n",
    "car_classifier.add(Dropout(dropout))\n",
    "\n",
    "#Flatten\n",
    "car_classifier.add(Flatten())\n",
    "\n",
    "#Adding Input and Output Layer\n",
    "car_classifier.add(Dense(units=layer_nodes,activation='relu'))\n",
    "car_classifier.add(Dense(units=layer_nodes,activation='relu'))\n",
    "car_classifier.add(Dense(units=layer_nodes,activation='relu'))\n",
    "car_classifier.add(Dense(units=616,activation='softmax'))\n",
    "\n",
    "car_classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 39783 images belonging to 616 classes.\n",
      "Found 12577 images belonging to 616 classes.\n"
     ]
    }
   ],
   "source": [
    "#Data agumentation\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_data = train_datagen.flow_from_directory('../scraped_images_2020/train',\n",
    "                                              target_size=(img_pixels,img_pixels),\n",
    "                                              batch_size=batchsize,\n",
    "                                              class_mode='categorical')\n",
    "test_data = test_datagen.flow_from_directory('../scraped_images_2020/test',\n",
    "                                              target_size=(img_pixels,img_pixels),\n",
    "                                              batch_size=batchsize,\n",
    "                                              class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 [==============================] - 1625s 16s/step - loss: 6.3145 - acc: 0.0059 - val_loss: 6.2833 - val_acc: 0.0067\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 1574s 16s/step - loss: 6.2809 - acc: 0.0069 - val_loss: 6.2722 - val_acc: 0.0068\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 1583s 16s/step - loss: 6.2637 - acc: 0.0068 - val_loss: 6.2359 - val_acc: 0.0068\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 1623s 16s/step - loss: 6.1592 - acc: 0.0072 - val_loss: 6.1163 - val_acc: 0.0074\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 1616s 16s/step - loss: 5.9998 - acc: 0.0091 - val_loss: 5.9304 - val_acc: 0.0118\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 1616s 16s/step - loss: 5.8212 - acc: 0.0133 - val_loss: 5.7652 - val_acc: 0.0150\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 1581s 16s/step - loss: 5.5895 - acc: 0.0223 - val_loss: 5.6815 - val_acc: 0.0208\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 1612s 16s/step - loss: 5.2977 - acc: 0.0366 - val_loss: 5.4804 - val_acc: 0.0266\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 1644s 16s/step - loss: 5.0177 - acc: 0.0531 - val_loss: 5.1235 - val_acc: 0.0457\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 1576s 16s/step - loss: 4.6815 - acc: 0.0780 - val_loss: 4.9096 - val_acc: 0.0619\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 1643s 16s/step - loss: 4.3967 - acc: 0.1039 - val_loss: 4.9295 - val_acc: 0.0695\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 1611s 16s/step - loss: 4.0575 - acc: 0.1415 - val_loss: 4.6731 - val_acc: 0.0972\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 1578s 16s/step - loss: 3.7969 - acc: 0.1758 - val_loss: 4.5365 - val_acc: 0.1099\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 1574s 16s/step - loss: 3.4708 - acc: 0.2195 - val_loss: 4.4696 - val_acc: 0.1281\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 1608s 16s/step - loss: 3.2822 - acc: 0.2482 - val_loss: 4.4224 - val_acc: 0.1438\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 1650s 16s/step - loss: 3.0114 - acc: 0.2907 - val_loss: 4.3891 - val_acc: 0.1608\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 1590s 16s/step - loss: 2.8237 - acc: 0.3209 - val_loss: 4.4888 - val_acc: 0.1668\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 1579s 16s/step - loss: 2.6217 - acc: 0.3546 - val_loss: 4.3196 - val_acc: 0.1833\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 1600s 16s/step - loss: 2.4581 - acc: 0.3849 - val_loss: 4.3704 - val_acc: 0.1949\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 1579s 16s/step - loss: 2.3164 - acc: 0.4084 - val_loss: 4.0878 - val_acc: 0.2273\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 1630s 16s/step - loss: 2.1592 - acc: 0.4405 - val_loss: 4.2744 - val_acc: 0.2242\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 1628s 16s/step - loss: 2.0655 - acc: 0.4557 - val_loss: 4.0496 - val_acc: 0.2512\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 1633s 16s/step - loss: 1.9404 - acc: 0.4811 - val_loss: 4.0481 - val_acc: 0.2692\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 1634s 16s/step - loss: 1.8753 - acc: 0.4907 - val_loss: 4.0582 - val_acc: 0.2697\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 1621s 16s/step - loss: 1.7509 - acc: 0.5195 - val_loss: 3.8765 - val_acc: 0.3069\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 1598s 16s/step - loss: 1.7075 - acc: 0.5275 - val_loss: 4.0268 - val_acc: 0.2886\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 1602s 16s/step - loss: 1.5923 - acc: 0.5539 - val_loss: 3.9658 - val_acc: 0.3048\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 1571s 16s/step - loss: 1.5718 - acc: 0.5551 - val_loss: 4.0997 - val_acc: 0.2936\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 1641s 16s/step - loss: 1.4939 - acc: 0.5720 - val_loss: 3.9077 - val_acc: 0.3396\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 1606s 16s/step - loss: 1.4580 - acc: 0.5794 - val_loss: 4.0161 - val_acc: 0.3248\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 1623s 16s/step - loss: 1.3925 - acc: 0.5959 - val_loss: 3.9482 - val_acc: 0.3339\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 1591s 16s/step - loss: 1.3645 - acc: 0.6006 - val_loss: 3.9939 - val_acc: 0.3502\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 1642s 16s/step - loss: 1.3121 - acc: 0.6109 - val_loss: 3.9333 - val_acc: 0.3501\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 1623s 16s/step - loss: 1.2767 - acc: 0.6205 - val_loss: 3.9823 - val_acc: 0.3582\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 1637s 16s/step - loss: 1.2395 - acc: 0.6277 - val_loss: 3.9021 - val_acc: 0.3646\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 1615s 16s/step - loss: 1.1955 - acc: 0.6382 - val_loss: 3.9636 - val_acc: 0.3743\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 1610s 16s/step - loss: 1.1765 - acc: 0.6438 - val_loss: 3.8881 - val_acc: 0.3741\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 1583s 16s/step - loss: 1.1395 - acc: 0.6517 - val_loss: 3.8192 - val_acc: 0.3989\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 1594s 16s/step - loss: 1.1478 - acc: 0.6489 - val_loss: 3.8964 - val_acc: 0.3700\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 1563s 16s/step - loss: 1.0695 - acc: 0.6679 - val_loss: 4.0215 - val_acc: 0.3745\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 1591s 16s/step - loss: 1.0860 - acc: 0.6635 - val_loss: 3.9450 - val_acc: 0.3789\n",
      "Epoch 42/50\n"
     ]
    }
   ],
   "source": [
    "history = car_classifier.fit_generator(train_data,\n",
    "                                       steps_per_epoch=steps_per_epoch,\n",
    "                                       epochs=epochs,\n",
    "                                       validation_data=test_data,\n",
    "                                       validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame.from_dict(history.history)\n",
    "metrics = pd.concat([pd.Series(range(0,30),name='epochs'),metrics],axis=1)\n",
    "metrics.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = car_classifier.to_json()\n",
    "with open(f\"../models/cars_classifier_tuned_model{model_num}.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "car_classifier.save_weights(f\"../models/cars_classifier_tuned_model{model_num}.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metrics\n",
    "metrics.to_csv(f\"../models/cars_classifier_metrics{model_num}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('models/cars_classifier_untuned.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"models/cars_classifier_untuned.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "test_image = image.load_img('../car_data/single_prediction/bmw_3series.jpg', target_size=(img_pixels, img_pixels))\n",
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to array and expand_dims because we are only doing 1 image prediction\n",
    "test_image_array = image.img_to_array(test_image)\n",
    "test_image_expand = np.expand_dims(test_image_array, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "classes = car_classifier.predict(test_image_expand, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List most likely predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "iterator = 0\n",
    "for key in train_data.class_indices:\n",
    "    results.setdefault(key, classes[0][iterator])\n",
    "    iterator+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results, orient='index').sort_values(0, ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model B: Classifying Make only\n",
    "\n",
    "<b>Aim</b>: Preidct the make of a car using the input image (e.g. BMW, Honda, etc).\n",
    "\n",
    "For this model, we will need to aggregate our existing training and prediction dataset into make ONLY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
